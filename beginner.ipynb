{
 "cells": [
  {
   "source": [
    "findspark: Spark' ı Python Script' lerinde kullanbilmeyi sağlayan yapı.\n",
    "\n",
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes. \n",
    "\n",
    "SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext [[Tutorials Points](https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm)]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"firtsapp\")\n",
    "# parametre\n",
    "# master: It is the URL of the cluster it connects to. (anlamadım)\n",
    "\n",
    "# Bu hücreyi bir kez daha çalıştırsanız şöyle bir hata alıcaksınız:\n",
    "# ValueError: Cannot run multiple SparkContexts at once"
   ]
  },
  {
   "source": [
    "Dizindeki README.md dosyasının içinde `a` ve `b` karakteri geçen satırların sayısını buluyoruz."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lines with a: 53, lines with b: 26\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"README.md\").cache()\n",
    "rowA = file.filter(lambda row: 'a' in row).count()\n",
    "rowB =file.filter(lambda row: 'b' in row).count()\n",
    "print(f\"Lines with a: {rowA}, lines with b: {rowB}\")"
   ]
  },
  {
   "source": [
    "##  PySpark RDD\n",
    "\n",
    "RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.\n",
    "\n",
    "RDDs are immutable elements, which means once you create an RDD you cannot change it. RDDs are fault tolerant as well, hence in case of any failure, they recover automatically.\n",
    "\n",
    "To apply operations on these RDD's, there are two ways −\n",
    "- Transformation and\n",
    "- Action\n",
    "\n",
    "Transformation − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "Action − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.\n",
    "\n",
    "To apply any operation in PySpark, we need to create a PySpark RDD first. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\n",
    "    \"scala\", \n",
    "    \"java\", \n",
    "    \"hadoop\", \n",
    "    \"spark\", \n",
    "    \"akka\",\n",
    "    \"spark vs hadoop\", \n",
    "    \"pyspark\",\n",
    "    \"pyspark and spark\"\n",
    "]\n",
    "words = sc.parallelize(arr) # RDD nesnesi oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# count(): RDD' de bulunan element' lerin sayısını döndürür.\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# collect(): RDD' deki bütün element' leri döndürür\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(): foreach fonksiyonuna parametre olarak verilen fonksiyonun şartını sağlayan elementleri dödürür.\n",
    "def f(x):\n",
    "    print(x)\n",
    "words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# filter(): Şartı sağlayan elemanlardan yeni bir RDD oluşturur\n",
    "filtered = words.filter(lambda x: 'spark' in x)\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('scala', 1),\n",
       " ('java', 1),\n",
       " ('hadoop', 1),\n",
       " ('spark', 1),\n",
       " ('akka', 1),\n",
       " ('spark vs hadoop', 1),\n",
       " ('pyspark', 1),\n",
       " ('pyspark and spark', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# map(): RDD' deki her bir element' e aynı işlemi uygulayarak yeni bir RDD döndürür.\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "words_map.collect()\n",
    "\n",
    "#for i in words_map.collect():\n",
    "#    print(i[0])\n",
    "# çalışıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\nscalajavahadoopsparkakkaspark vs hadooppysparkpyspark and spark\n"
     ]
    }
   ],
   "source": [
    "# reduce(): Kendisine parametre olarak verilen işlemi gerçekleştirir.\n",
    "from operator import add\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(nums.reduce(add))\n",
    "print(words.reduce(add))\n",
    "\n",
    "#def myAdd(x):\n",
    "#    filtered_arr = list(filter(lambda i: \"spark\" in i, x.collect()))\n",
    "#    return ''.join(filtered_arr)\n",
    "\n",
    "#words.reduce(myAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('hadoop', (4, 5)), ('spark', (1, 2))]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "# join():\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4), (\"dene\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5), (\"naber\", 4)])\n",
    "joined = x.join(y)\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}