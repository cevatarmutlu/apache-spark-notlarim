{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Overview](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview)\n",
    "\n",
    "**High level** her Spark uygulaması ***driver program*** denen bir şeyden oluşur. Bu ***driver program*** kullanıcın *main* fonksiyonunu çalıştırır ve **cluster** üzerinde çeşitli paralel işlemleri execute eder.\n",
    "\n",
    "Spark' ın sağladığı main **absraction** ***resilient distributed dataset*** (RDD) olmaktadır. RDD, bir elemanlar kümesidir(collection of elements). RDD, cluster üzerindeki node' lar boyunca partition' lara (bölümlere) ayrılmıştır. RDD' lerin node' lara bölünmesi sayesinde paralel işlemler yapılabilmektedir.\n",
    "\n",
    "RDD' ler çeşitli formattaki dosyalardan ya da driver program içindeki collectionlardan oluşturulabilir (collectiondan kasıt sanırım scala dizileri ya da hangi dili kullanıyorsanız).\n",
    "\n",
    "Eğer bir RDD' yi çok sık şekilde kullanacaksanız Spark' a bu RDD' yi cache al diyebiliyorsunuz.\n",
    "\n",
    "RDD' ler node' larda oluşan hatalardan otomatik olarak recover edilirler Sanırım hiç hata olmamış hallerine geri döndürülürler.\n",
    "\n",
    "___\n",
    "\n",
    "Spark' ın ikinci absraction' u paralel işlemler kullanılan ***shared variables*** tir.\n",
    "\n",
    "Sanırım Spark' ta Default olarak şöyle bir şey gerçekleştiriliyor: Spark' ta paralel bir işlem başladığında node' lara task' lar dağıtılıyor. Bu task' lar için gerekli olan variable' lar node' larda çalışacak olan fonksiyona kopyalanıyor.\n",
    "\n",
    "Bazen bir tane değişkenin tasklar arasında ya da task ile driver program arasında shared edilmesi gerekiyormuş. Bu özelliğin var olma sebebi bu.\n",
    "\n",
    "Sparkta iki tane shared variables türü varmış:\n",
    "***broadcast variables***: Tüm nodeların memory' lerindeki bir value' yi cache almak için kullanılır. (Ne demek bilmiyorum.)\n",
    "***accumulators***: Yanlızca \"added\" değişkenlermiş. Örneğin counters ve sum. (Ne demek bilmiyorum as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Resilient Distributed Datasets (RDDs)](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "\n",
    "Spark ***Resilient Distributed Datasets*** kavramı etrafında döner. RDD, paralel olarak çalıştırılabilen, hata toleranslı elementler koleksiyonudur (**fault-tolerant** collection of elements).\n",
    "\n",
    "RDD iki şekilde oluşturulur: <br/>\n",
    "birincisi driver programda bulunan colloection' u ***parallelizing*** ederek. <br/>\n",
    "ikincisi external storage system' da bulunan bir dataset' e referans vererek. external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Collections\n",
    "\n",
    "Bir parallelized collection, sizin driver programınızda bulunan collection' ı parametre olarak olan ***SparkContext***' in ***parallelize*** metodu ile oluşturulur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Array[Int] = Array(1, 2, 3, 4, 5)\n",
       "distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data) //RDD oluştu Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parruldu \n",
    "// ve paralel işlem yapılabilir artık."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel collection' larda önemli bir konuda bu RDD' nin kaç tane partition' a bölüneceği. Spark normalde partion' lama işlemlerini kendi yaparmış. Ama elle biz de verebilirmişiz. Elle vermek istersek `sc.parallelize(data, 10)` bu şekilde yapabiliriz.\n",
    "\n",
    "> Bazı yerlerde `partions` yerine `slices` ifadesi geçebilirmiş."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
